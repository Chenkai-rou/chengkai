import streamlit as st
from openai import OpenAI
from gtts import gTTS
from docx import Document
from io import BytesIO
import easyocr
from PIL import Image
import numpy as np
from duckduckgo_search import DDGS
from pypdf import PdfReader
from streamlit_mic_recorder import speech_to_text

# --- 1. é¡µé¢è®¾ç½®ï¼šå›å½’æç®€ ---
st.set_page_config(
    page_title="ç¨‹å‡¯çš„å…¨èƒ½åŠ©æ‰‹", 
    page_icon="ğŸ¤–", 
    layout="wide", 
    initial_sidebar_state="expanded"
)

st.title("ğŸ¤– ç¨‹å‡¯çš„å…¨èƒ½åŠ©æ‰‹")
st.write("è”ç½‘æœç´¢ | æ–‡æ¡£åˆ†æ | è¯­éŸ³äº¤äº’ | è§†è§‰è¯†åˆ«")
st.divider() # åŠ ä¸€æ¡ä¼˜é›…çš„åˆ†éš”çº¿

# --- 2. åˆå§‹åŒ–æ¨¡å‹ (ç¼“å­˜) ---
@st.cache_resource
def load_ocr_model():
    return easyocr.Reader(['ch_sim', 'en'], gpu=False)

with st.spinner("æ­£åœ¨å¯åŠ¨è§†è§‰å¼•æ“..."):
    ocr_reader = load_ocr_model()

# --- 3. è·å– API Key ---
try:
    api_key = st.secrets["DEEPSEEK_API_KEY"]
except:
    api_key = st.sidebar.text_input("è¯·è¾“å…¥ DeepSeek API Key", type="password")

# --- 4. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def search_web(query):
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=3))
            if results:
                return "\n".join([f"- {r['title']}: {r['body']}" for r in results])
        return "æœªæœç´¢åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
    except Exception as e:
        return f"æœç´¢æš‚ä¸å¯ç”¨: {e}"

def read_pdf(file):
    reader = PdfReader(file)
    text = ""
    for page in reader.pages[:5]: # åªè¯»å‰5é¡µ
        text += page.extract_text()
    return text

def speak_text(text):
    try:
        short_text = text[:100] # åªè¯»å‰100å­—
        tts = gTTS(text=short_text, lang='zh-cn')
        tts.save("response.mp3")
        audio_file = open("response.mp3", "rb")
        audio_bytes = audio_file.read()
        st.audio(audio_bytes, format="audio/mp3", autoplay=True)
    except:
        pass

# --- 5. ä¾§è¾¹æ ï¼šæ¸…çˆ½çš„åŠŸèƒ½åŒº ---
st.sidebar.header("ğŸ› ï¸ åŠŸèƒ½é¢æ¿")

# [æ’ä»¶ 1] è”ç½‘å¼€å…³
enable_search = st.sidebar.toggle("ğŸŒ å¼€å¯è”ç½‘æœç´¢")

# [æ’ä»¶ 2] çŸ¥è¯†åº“
st.sidebar.write("---")
uploaded_file = st.sidebar.file_uploader("ğŸ“„ ä¸Šä¼ æ–‡æ¡£ (PDF)", type=["pdf"])
knowledge_base = ""
if uploaded_file:
    knowledge_base = read_pdf(uploaded_file)
    st.sidebar.success(f"å·²è¯»å–: {uploaded_file.name}")

# [æ’ä»¶ 3] æ‹ç…§è¯†å›¾
st.sidebar.write("---")
with st.sidebar.expander("ğŸ“· æ‹ç…§/è¯†å›¾"): # ç”¨æŠ˜å å¡ç‰‡æ”¶çº³ï¼Œæ›´æ•´æ´
    img_input = st.camera_input("ç‚¹å‡»æ‹ç…§")
    ocr_text = ""
    if img_input:
        image = Image.open(img_input)
        img_array = np.array(image)
        results = ocr_reader.readtext(img_array)
        ocr_text = "\n".join([res[1] for res in results])
        st.info(f"è¯†åˆ«å†…å®¹ï¼š{ocr_text[:50]}...")

# [æ’ä»¶ 4] è¯­éŸ³å¯¹è®²
st.sidebar.write("---")
st.sidebar.write("ğŸ¤ è¯­éŸ³è¾“å…¥")
audio_text = speech_to_text(language='zh', start_prompt="ç‚¹å‡»è¯´è¯", stop_prompt="ç‚¹å‡»å‘é€", just_once=True)

# --- 6. èŠå¤©é€»è¾‘ ---
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ä½ å¥½ï¼Œç¨‹å‡¯ã€‚ç³»ç»Ÿå·²å°±ç»ªï¼Œéšæ—¶å¾…å‘½ã€‚"}]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

# å¤„ç†è¾“å…¥
final_user_input = None
if audio_text:
    final_user_input = audio_text
elif ocr_text:
    final_user_input = f"ã€å›¾ç‰‡å†…å®¹ã€‘ï¼š\n{ocr_text}"
else:
    text_input = st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜...")
    if text_input:
        final_user_input = text_input

# --- 7. ç”Ÿæˆå›å¤ ---
if final_user_input:
    if not api_key:
        st.error("è¯·å…ˆé…ç½®å¯†é’¥")
    else:
        st.session_state.messages.append({"role": "user", "content": final_user_input})
        with st.chat_message("user"):
            st.write(final_user_input)

        # æ„å»º Prompt
        context_info = ""
        if enable_search:
            with st.status("ğŸ” æ­£åœ¨æ£€ç´¢ç½‘ç»œ...", expanded=False) as status:
                search_result = search_web(final_user_input)
                context_info += f"\n\nã€æœç´¢ç»“æœã€‘ï¼š\n{search_result}\n"
                status.update(label="æœç´¢å®Œæˆ", state="complete")
        
        if knowledge_base:
            context_info += f"\n\nã€æ–‡æ¡£å†…å®¹ã€‘ï¼š\n{knowledge_base[:2000]}...\n"

        client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
        
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·ï¼š
        {context_info}
        å¦‚æœä½ æœ‰æ–‡æ¡£æˆ–æœç´¢ç»“æœï¼Œè¯·ä¼˜å…ˆå‚è€ƒã€‚ä¿æŒå›ç­”ç®€æ´ã€ä¸“ä¸šã€‚
        """

        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸­..."):
                try:
                    response = client.chat.completions.create(
                        model="deepseek-chat",
                        messages=[{"role": "system", "content": system_prompt}, *st.session_state.messages]
                    )
                    result = response.choices[0].message.content
                    st.write(result)
                    st.session_state.messages.append({"role": "assistant", "content": result})
                    speak_text(result)
                except Exception as e:
                    st.error(f"Error: {e}")